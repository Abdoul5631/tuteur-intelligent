"""
Service d'intÃ©gration LLM (Large Language Model)
GÃ¨re OpenAI, Gemini et autres providers
"""

import os
import json
from typing import Optional, List, Dict, Any
from abc import ABC, abstractmethod
from enum import Enum

from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Pour suppression : from openai import OpenAI
# Pour suppression : import google.generativeai as genai

# DÃ©fensifs : Ã©viter que Pylance signale les imports manquants en l'Ã©diteur
try:
    from pydantic import BaseModel  # pragma: no cover
except Exception:  # pragma: no cover
    class BaseModel:  # simple stub pour l'Ã©diteur / tests lÃ©gers
        pass

try:
    from dotenv import load_dotenv  # pragma: no cover
except Exception:  # pragma: no cover
    def load_dotenv():  # stub
        return None

try:
    import openai  # pragma: no cover
except Exception:  # pragma: no cover
    openai = None

import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=FutureWarning)
    try:
        import google.generativeai as generativeai  # pragma: no cover
        from google.generativeai import types as generative_types  # pragma: no cover
    except Exception:  # pragma: no cover
        generativeai = None
        generative_types = None

load_dotenv()


class LLMProvider(str, Enum):
    """Providers disponibles"""
    OPENAI = "openai"
    GEMINI = "gemini"
    OLLAMA = "ollama"  # Local


class NiveauScolaire(str, Enum):
    """Niveaux scolaires"""
    CP1 = "cp1"
    CP2 = "cp2"
    CE1 = "ce1"
    CE2 = "ce2"
    CM1 = "cm1"
    CM2 = "cm2"
    SIXIEME = "6eme"
    CINQUIEME = "5eme"
    QUATRIEME = "4eme"
    TROISIEME = "3eme"
    SECONDE = "seconde"
    PREMIERE = "1ere"
    TERMINALE = "terminale"


class Matiere(str, Enum):
    """MatiÃ¨res disponibles"""
    MATHEMATIQUES = "mathematiques"
    FRANCAIS = "francais"
    HISTOIRE_GEO = "histoire_geo"
    SCIENCES = "sciences"
    ANGLAIS = "anglais"
    SCIENCES_VIE = "sciences_vie"
    PHYSIQUE_CHIMIE = "physique_chimie"
    TECHNOLOGIE = "technologie"
    EPS = "eps"
    ARTS = "arts"


class ExerciseType(str, Enum):
    """Types d'exercices"""
    CHOIX_MULTIPLE = "choix_multiple"
    REPONSE_COURTE = "reponse_courte"
    REDACTION = "redaction"
    CALCUL = "calcul"
    VRAI_FAUX = "vrai_faux"
    MATCHING = "matching"


# ========================
# PROMPT TEMPLATES
# ========================

SYSTEM_PROMPT_TUTEUR = """Tu es un tuteur intelligent et bienveillant pour Ã©lÃ¨ves de primaire et secondaire.

**Contexte PÃ©dagogique:**
- Niveau scolaire: {niveau}
- MatiÃ¨re: {matiere}
- Ã‚ge estimÃ©: {age} ans
- Points forts de l'Ã©lÃ¨ve: {strengths}
- Points Ã  amÃ©liorer: {weak_areas}
- LeÃ§on en cours (si l'Ã©lÃ¨ve est sur une leÃ§on): {lecon_titre}
- Contenu de la leÃ§on (extrait pour contextualiser): {lecon_contenu}

**Directives Essentielles:**
1. ðŸŽ¯ ADAPTE ton langage au niveau de l'Ã©lÃ¨ve:
   - CP1-CE2: TrÃ¨s simple, mots courants, phrases courtes
   - CM1-CM2: Langage clair, quelques termes techniques avec explications
   - 6Ã¨me-3Ã¨me: Plus dÃ©taillÃ©, dÃ©finitions prÃ©cises
   - LycÃ©e: Technique, nuancÃ©, approfondi

2. ðŸ’¡ Utilise des analogies et exemples concrets:
   - Pour la fraction: "C'est comme un gÃ¢teau divisÃ©"
   - Pour les verbes: "C'est l'action que fait quelqu'un"

3. âœ… Soit encourageant et positif TOUJOURS

4. â“ Pose des questions pour vÃ©rifier la comprÃ©hension

5. âš ï¸ N'utilise PAS de jargon technique pas expliquÃ© pour niveaux bas

6. ðŸ“ Structure tes rÃ©ponses clairement:
   - Salutation courte
   - Explication simple
   - Exemple(s) concret(s)
   - VÃ©rification comprÃ©hension
   - Suggestion d'exercice si pertinent

7. ðŸŽ“ Adapte Ã  son style d'apprentissage

**RÃ©ponse attendue (JSON):**
{{
    "reponse": "...",
    "type": "explication|question|exercice|feedback",
    "niveau_adapte": true,
    "confiance": 0.95
}}
"""

SYSTEM_PROMPT_EXERCICE_GENERATOR = """Tu es un gÃ©nÃ©rateur d'exercices pÃ©dagogiques intelligent.

**Contexte:**
- Niveau: {niveau}
- MatiÃ¨re: {matiere}
- Nombre d'exercices Ã  gÃ©nÃ©rer: {nombre}
- Topics: {topics}
- DifficultÃ©s antÃ©rieures: {difficulty_history}

**RÃ¨gles de gÃ©nÃ©ration:**
1. GÃ©nÃ¨re des exercices variÃ©s et intÃ©ressants
2. Adapte la difficultÃ© au niveau de l'Ã©lÃ¨ve
3. BasÃ© sur les lacunes identifiÃ©es
4. Inclus des problÃ¨mes du monde rÃ©el quand possible
5. Pour chaque exercice, fournis:
   - Question claire
   - Options (si choix multiple)
   - RÃ©ponse correcte
   - 2-3 erreurs courantes possibles
   - Explication dÃ©taillÃ©e

**Format de rÃ©ponse (JSON):**
{{
    "exercices": [
        {{
            "id": 1,
            "type": "choix_multiple|reponse_courte|calcul",
            "question": "...",
            "options": ["a", "b", "c"],  // optionnel
            "reponse": "...",
            "explication": "...",
            "erreurs_courantes": ["...", "..."],
            "difficulte": 5,
            "temps_estime": 180,
            "points": 10
        }}
    ],
    "note_generation": {{
        "qualite": 0.95,
        "adaptation": "bonne",
        "variete": "excellente"
    }}
}}
"""

SYSTEM_PROMPT_ANALYSIS = """Tu es un analyste pÃ©dagogique IA.

Analyse la rÃ©ponse de l'Ã©lÃ¨ve et fournis un feedback dÃ©taillÃ©.

**RÃ©ponse attendue:**
{{
    "correct": true|false,
    "score": 0-100,
    "feedback_positif": "...",
    "raison_erreur": "...",
    "explication": "...",
    "prochaine_etape": {{
        "type": "exercice_similaire|lecon_preparatoire|approfondissement",
        "titre": "..."
    }},
    "encouragement": "..."
}}
"""


# ========================
# MODELS PYDANTIC
# ========================

class ChatMessage(BaseModel):
    """ModÃ¨le pour message de chat"""
    role: str = Field(..., description="user ou assistant")
    content: str = Field(..., description="Contenu du message")


class GeneratedExercise(BaseModel):
    """ModÃ¨le pour exercice gÃ©nÃ©rÃ©"""
    type: ExerciseType
    question: str
    options: Optional[List[str]] = None
    reponse: str
    explication: str
    erreurs_courantes: List[str]
    difficulte: int = Field(ge=1, le=10)
    temps_estime: int = Field(description="en secondes")
    points: int = Field(default=10)


class AnalysisResult(BaseModel):
    """ModÃ¨le pour analyse de rÃ©ponse"""
    correct: bool
    score: int = Field(ge=0, le=100)
    feedback_positif: str
    raison_erreur: Optional[str] = None
    explication: str
    prochaine_etape: Optional[Dict[str, str]] = None
    encouragement: str


# ========================
# BASE SERVICE
# ========================

class BaseLLMService(ABC):
    """Service LLM abstrait"""

    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model

    @abstractmethod
    def chat(self, messages: List[ChatMessage], system_prompt: str) -> str:
        """Chat avec le modÃ¨le"""
        pass

    @abstractmethod
    def generate_text(self, prompt: str) -> str:
        """GÃ©nÃ©rer du texte"""
        pass


# ========================
# OPENAI SERVICE (EN ATTENTE DE CLÃ‰ API)
# ========================

class OpenAIService(BaseLLMService):
    """Service OpenAI (nÃ©cessite clÃ© API)"""
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo"):
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        if not api_key:
            # Pour respecter l'exigence: le backend doit appeler OpenAI Ã  chaque rÃ©ponse.
            # Si la clÃ© n'est pas fournie, on lÃ¨ve une erreur lors de l'initialisation pour forcer la configuration.
            raise EnvironmentError("OPENAI_API_KEY non configurÃ©e. DÃ©finissez la variable d'environnement OPENAI_API_KEY.")

        try:
            from openai import OpenAI
        except Exception as e:
            raise ImportError("Package 'openai' requis mais non installÃ©: pip install openai") from e

        # Utiliser la nouvelle API OpenAI v1.0.0+ (initialiser sans proxies)
        self.client = OpenAI(api_key=api_key)

    def chat(self, messages: List[Dict], system_prompt: str) -> str:
        """Chat avec OpenAI en envoyant un system prompt puis les messages fournis.

        Retourne la chaÃ®ne de texte produite par le modÃ¨le. Le modÃ¨le est encouragÃ©
        Ã  produire du JSON conforme aux prompts systÃ¨mes (`SYSTEM_PROMPT_TUTEUR` / `SYSTEM_PROMPT_EXERCICE_GENERATOR`).
        """
        # Construire la liste de messages au format API OpenAI
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})

        # Messages doivent Ãªtre des dicts {role, content}
        for m in messages:
            # Normaliser les rÃ´les en 'user' ou 'assistant'
            role = m.get('role', 'user')
            content = m.get('content', '')
            full_messages.append({"role": role, "content": content})

        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=full_messages,
                temperature=0.3,
                max_tokens=1500
            )
            # Nouvelle API retourne un objet avec .content
            return resp.choices[0].message.content
        except Exception as e:
            # Ne jamais renvoyer l'input utilisateur en cas d'erreur
            raise RuntimeError(f"Erreur OpenAI: {e}") from e

    def generate_text(self, prompt: str) -> str:
        return self.chat([{"role": "user", "content": prompt}], "")


# ========================
# GEMINI SERVICE (EN ATTENTE DE CLÃ‰ API)
# ========================

class GeminiService(BaseLLMService):
    """Service Google Gemini (nÃ©cessite clÃ© API)"""

    def __init__(self, api_key: Optional[str] = None):
        api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not api_key:
            self.available = False
            print("âš ï¸ GEMINI_API_KEY non configurÃ©e - Mode dÃ©mo")
        else:
            self.available = True
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-pro')
            except ImportError:
                print("âš ï¸ Package 'google-generativeai' non installÃ©")
                self.available = False

    def chat(self, messages: List[Dict], system_prompt: str) -> str:
        """Chat avec Gemini"""
        if not self.available:
            return self._mock_response()

        try:
            from google.generativeai.types import ContentType

            conversation = self.model.start_chat()
            
            # PrÃ©parer le contexte
            context = f"{system_prompt}\n\nConversation:\n"
            for msg in messages[:-1]:
                role = "User" if msg["role"] == "user" else "AI"
                context += f"{role}: {msg['content']}\n"

            response = conversation.send_message(
                messages[-1]["content"] if messages else "Bonjour"
            )
            return response.text
        except Exception as e:
            print(f"âŒ Erreur Gemini: {e}")
            return self._mock_response()

    def generate_text(self, prompt: str) -> str:
        """GÃ©nÃ©rer du texte"""
        if not self.available:
            return self._mock_response()
        return self.chat([{"role": "user", "content": prompt}], "")

    def _mock_response(self) -> str:
        """RÃ©ponse mock pour dÃ©mo"""
        return json.dumps({
            "reponse": "RÃ©ponse en mode dÃ©mo (configurez GEMINI_API_KEY)",
            "type": "explication",
            "niveau_adapte": True,
            "confiance": 0.5
        })


# ========================
# MOCK SERVICE (POUR TESTS)
# ========================

class MockLLMService(BaseLLMService):
    """Service Mock intelligent pour tests et dÃ©mo - VRAIE logique pÃ©dagogique"""

    def __init__(self):
        self.conversation_history = {}
        
        # Contenu pÃ©dagogique intelligent par niveau et matiÃ¨re
        self.explications = {
            "CM1-CM2": {
                "mathÃ©matiques": {
                    "fraction": "Une fraction, c'est comme diviser quelque chose en parts Ã©gales. Par exemple, si tu coupes un gÃ¢teau en 4 parts Ã©gales et tu en prends 1, tu as 1/4 du gÃ¢teau!",
                    "pÃ©rimÃ¨tre": "Le pÃ©rimÃ¨tre, c'est la distance autour d'une forme. Imagine du scotch qui entoure un carrÃ©: la longueur du scotch, c'est le pÃ©rimÃ¨tre!",
                    "aire": "L'aire, c'est l'espace Ã  l'intÃ©rieur d'une forme. Imagine peindre le sol d'une piÃ¨ce: la quantitÃ© de peinture dÃ©pend de l'aire!",
                    "dÃ©cimales": "Les dÃ©cimales, ce sont les chiffres aprÃ¨s la virgule. Comme l'argent: 2,50â‚¬, c'est 2 euros et 50 centimes!"
                },
                "franÃ§ais": {
                    "verbe": "Un verbe, c'est un mot qui montre une action. Courir, sauter, manger, lire... ce sont des verbes!",
                    "adjectif": "Un adjectif dÃ©crit une personne ou une chose. Grand, petit, bleu, heureux... ce sont des adjectifs!",
                    "accord": "L'accord, c'est faire en sorte que les mots 'se parlent' entre eux. Si tu dis 'un chat noir', 'noir' s'accorde avec 'chat'!",
                    "homophones": "Ce sont des mots qui se prononcent pareil mais n'ont pas le mÃªme sens. Exemple: 'est' (verbe Ãªtre) et 'et' (conjonction)!"
                }
            },
            "6Ã¨me-3Ã¨me": {
                "mathÃ©matiques": {
                    "Ã©quation": "Une Ã©quation, c'est trouver la valeur mystÃ©rieuse (souvent x). Par exemple: 2x + 3 = 7. On doit trouver x!",
                    "thÃ©orÃ¨me": "Un thÃ©orÃ¨me, c'est une rÃ¨gle mathÃ©matique importante. Pythagore, c'est un exemple cÃ©lÃ¨bre!",
                    "probabilitÃ©": "La probabilitÃ© mesure la chance qu'un Ã©vÃ©nement arrive. Entre 0 (impossible) et 1 (certain).",
                    "fonction": "Une fonction, c'est une machine: tu rentre un nombre, elle te ressort un autre nombre suivant une rÃ¨gle."
                },
                "franÃ§ais": {
                    "subordonnÃ©e": "Une proposition subordonnÃ©e dÃ©pend d'une propositionprincipale. Elle commence souvent par 'qui', 'que', 'parce que'...",
                    "conjugaison": "Changer la forme du verbe selon la personne et le temps. Je suis, Tu es, Il est...",
                    "analyse": "Ã‰tudier chaque mot pour comprendre la structure d'une phrase ou l'intention d'un auteur.",
                    "littÃ©rature": "L'Ã©tude des Å“uvres Ã©crites pour les analyser et les interprÃ©ter!"
                }
            },
            "2nde-Tle": {
                "mathÃ©matiques": {
                    "dÃ©rivÃ©e": "La dÃ©rivÃ©e mesure comment une fonction change. C'est la pente de la courbe Ã  un point donnÃ©.",
                    "intÃ©grale": "L'intÃ©grale calcule l'aire sous une courbe. C'est l'inverse de la dÃ©rivÃ©e!",
                    "logarithme": "Le log sert Ã  rÃ©soudre des Ã©quations exponentielles: log(a^x) = x * log(a).",
                    "limite": "La limite c'est vers quel nombre on tend quand on s'approche d'un point."
                },
                "philosophie": {
                    "Ã©pistÃ©mologie": "C'est l'Ã©tude de comment on connaÃ®t les choses. Qu'est-ce que la science?",
                    "morale": "L'Ã©tude du bien et du mal, de comment on doit se comporter.",
                    "logique": "L'art de raisonner correctement: prÃ©misses â†’ conclusion.",
                    "mÃ©tapysique": "L'Ã©tude de la rÃ©alitÃ© ultime: qu'est-ce qui existe vraiment?"
                }
            }
        }

    def _analyser_historique(self, messages: List[Dict], niveau: str, matiere: str) -> Dict[str, Any]:
        """
        Analyser l'historique de conversation pour extraire le contexte
        DÃ©tecte : Ã©tait-ce une question? une demande d'exercice? etc.
        """
        contexte = {
            "est_reponse_question": False,
            "question_precedente": None,
            "contexte_matiere_identifie": False,
            "nombre_echanges": len(messages),
            "concepts_mentionnes": []
        }
        
        if len(messages) < 2:
            return contexte
        
        # Analyser les messages prÃ©cÃ©dents pour trouver la question posÃ©e
        for i in range(len(messages) - 2, -1, -1):  # De l'avant-avant-dernier vers le dÃ©but
            msg = messages[i]
            if msg.get("role") == "assistant":
                # Chercher si c'Ã©tait une question Ã  l'utilisateur
                contenu = msg.get("content", "").lower()
                if any(word in contenu for word in ["veux-tu", "veux tu", "?", "veux"]):
                    contexte["est_reponse_question"] = True
                    contexte["question_precedente"] = msg.get("content", "")
                    break
        
        return contexte

    def _generer_reponse_intelligente(
        self,
        message_courant: str,
        contexte: Dict[str, Any],
        niveau: str,
        matiere: str,
        system_prompt: str
    ) -> str:
        """GÃ©nÃ©rer une rÃ©ponse vraiment intelligente basÃ©e sur le contexte rÃ©el"""
        
        # Si c'est une rÃ©ponse positive ou nÃ©gative
        prompt_lower = message_courant.lower().strip()
        
        if prompt_lower in ["oui", "ouais", "yep", "yes", "d'accord", "ok", "ok!", "oki"]:
            # Analyser ce qu'on proposait dans le message prÃ©cÃ©dent
            prev_question = contexte.get("question_precedente", "").lower()
            
            if any(word in prev_question for word in ["exercice", "s'entraÃ®ner", "pratiquer"]):
                n = 2
                exercices = self._generer_exercices_contextuels(n, niveau, matiere)
                return json.dumps({
                    "exercices": exercices,
                    "reponse": f"ðŸŽ¯ Parfait! Voici {n} exercices sur {matiere} adaptÃ©s au niveau {niveau}:",
                    "type": "exercice",
                    "niveau_adapte": True,
                    "confiance": 0.95
                })
            
            elif any(word in prev_question for word in ["continuer", "plusque", "suite", "approfondir"]):
                return json.dumps({
                    "reponse": f"Excellent! On va approfondir {matiere}. Dis-moi sur quel concept tu aimerais que je t'aide: fractions, Ã©quations, littÃ©rature... ?",
                    "type": "explication",
                    "niveau_adapte": True,
                    "confiance": 0.9
                })
            else:
                return json.dumps({
                    "reponse": f"Super! Je suis ravi que le soit compris. On va continuer? Tu peux me demander une explication, un exercice, ou m'aider sur un concept prÃ©cis en {matiere}.",
                    "type": "explication",
                    "niveau_adapte": True,
                    "confiance": 0.85
                })
        
        elif prompt_lower in ["non", "non pas", "nope", "non merci", "non!", "pas"]:
            return json.dumps({
                "reponse": "D'accord! Pas de problÃ¨me. Qu'est-ce que je peux faire pour t'aider? Je peux expliquer un concept, te proposer un exercice diffÃ©rent, ou mÃªme une rÃ©vision.",
                "type": "explication",
                "niveau_adapte": True,
                "confiance": 0.85
            })
        
        # Fallback
        return json.dumps({
            "reponse": f"Je vois. En {matiere}, je peux t'aider avec des explications, des exercices, ou des rÃ©visions. Qu'est-ce que tu prÃ©fÃ¨res?",
            "type": "explication",
            "niveau_adapte": True,
            "confiance": 0.8
        })

    def _extraire_niveau_depuis_prompt(self, prompt: str) -> str:
        """Extraire le niveau depuis le system prompt"""
        if "CP1" in prompt or "CE" in prompt or "CM" in prompt:
            if "CM1" in prompt or "CM2" in prompt:
                return "CM1-CM2"
            return "CP1-CE2"
        elif "6" in prompt and "Ã¨me" in prompt or "3" in prompt and "Ã¨me" in prompt:
            return "6Ã¨me-3Ã¨me"
        elif "2nde" in prompt or "Tale" in prompt or "LycÃ©e" in prompt:
            return "2nde-Tle"
        return "CM1-CM2"  # dÃ©faut

    def _extraire_matiere_depuis_prompt(self, prompt: str) -> str:
        """Extraire la matiÃ¨re depuis le system prompt"""
        matiere_map = {
            "mathÃ©matiques": "mathÃ©matiques",
            "math": "mathÃ©matiques",
            "franÃ§ais": "franÃ§ais",
            "francais": "franÃ§ais",
            "franÃ§ais": "franÃ§ais",
            "svt": "svt",
            "sciences": "svt",
            "physique": "physique",
            "chimie": "physique",
            "philosophie": "philosophie",
            "philo": "philosophie",
            "histoire": "histoire",
            "geo": "histoire"
        }
        prompt_lower = prompt.lower()
        for mot, matiere in matiere_map.items():
            if mot in prompt_lower:
                return matiere
        return "gÃ©nÃ©ral"

    def chat(self, messages: List[Dict], system_prompt: str) -> str:
        """Chat intelligent avec contexte pÃ©dagogique rÃ©el et historique de conversation"""
        
        if not messages:
            return json.dumps({
                "reponse": "Bonjour! Je suis ton tuteur intelligent. Je m'adapte Ã  ton niveau et je suis lÃ  pour clarifier tes doutes. Qu'est-ce que tu aimerais apprendre?",
                "type": "salutation",
                "niveau_adapte": True,
                "confiance": 0.95
            })

        # ðŸ”¥ RÃ©cupÃ©rer le dernier message (message courant de l'utilisateur)
        last_msg = messages[-1].get("content", "").strip() if messages else ""
        prompt_lower = last_msg.lower()
        
        # Extraire le contexte pÃ©dagogique du system prompt
        niveau = self._extraire_niveau_depuis_prompt(system_prompt)
        matiere = self._extraire_matiere_depuis_prompt(system_prompt)

        # ðŸ”¥ NOUVEAU : Analyser l'historique complet pour mieux comprendre le contexte
        contexte_conversation = self._analyser_historique(messages, niveau, matiere)

        # ============ SALUTATION / SIMPLE REFLEXE ==============
        if len(messages) == 1:  # Premier message
            if any(word in prompt_lower for word in ["bonjour", "hello", "salut", "coucou", "hi"]):
                return json.dumps({
                    "reponse": f"Salut! Bienvenue! Je suis ton tuteur en {matiere}. Je suis lÃ  pour t'aider Ã  mieux comprendre. Qu'est-ce que tu aimerais apprendre ou dont tu as besoin d'aide?",
                    "type": "salutation",
                    "niveau_adapte": True,
                    "confiance": 0.95
                })

        # ============ RÃ‰PONSE Ã€ UNE QUESTION PRÃ‰CÃ‰DENTE ==============
        # Si contexte_conversation a identifiÃ© qu'on rÃ©pond Ã  une question
        if contexte_conversation.get("est_reponse_question"):
            return self._generer_reponse_intelligente(
                last_msg,
                contexte_conversation,
                niveau,
                matiere,
                system_prompt
            )

        # ============ GESTION EXPLICITE DE CONCEPTS ==============
        for concept_key, explications_dict in self.explications.get(niveau, {}).get(matiere, {}).items():
            if concept_key in prompt_lower:
                return json.dumps({
                    "reponse": f"Bonne question sur {concept_key}! {explications_dict}\n\nMaintenant que tu comprends Ã§a, tu veux essayer un exercice?",
                    "type": "explication",
                    "niveau_adapte": True,
                    "confiance": 0.95
                })

        # ============ GÃ‰NÃ‰RATION D'EXERCICES INTELLIGENTS ==============
        import re
        
        if any(word in prompt_lower for word in ["exercice", "exercices", "gÃ©nÃ¨r", "genere", "gen", "entrainement", "entraÃ®nement", "pratique"]):
            m = re.search(r"(\d+)", last_msg)
            n = int(m.group(1)) if m else 3
            n = min(10, max(1, n))
            
            exercices = self._generer_exercices_contextuels(n, niveau, matiere)
            
            return json.dumps({
                "exercices": exercices,
                "note_generation": {
                    "qualite": 0.95,
                    "adaptation": f"adaptÃ© au niveau {niveau}",
                    "nombre": n,
                    "matiere": matiere
                }
            })

        # ============ ANALYSE DE RÃ‰PONSE INTELLIGENTE ==============
        
        if any(word in prompt_lower for word in ["corrig", "analyse", "reponse", "rÃ©ponse", "verifi", "juste", "faux"]):
            est_correcte = self._evaluer_reponse_basique(last_msg, prompt_lower)
            score = 85 if est_correcte else 35
            
            return json.dumps({
                "correct": est_correcte,
                "score": score,
                "feedback_positif": "âœ“ Tu as fait un bonne effort!" if est_correcte else "Tu es sur la bonne voie, mais il manque quelque chose.",
                "raison_erreur": None if est_correcte else "Attention Ã  la mÃ©thode - relis le concept clÃ©.",
                "explication": "Tu as bien appliquÃ© la mÃ©thode!" if est_correcte else "Essaie de vÃ©rifier chaque Ã©tape de ton raisonnement.",
                "prochaine_etape": {
                    "type": "approfondissement" if est_correcte else "exercice_similaire",
                    "titre": "Exercice d'approfondissement" if est_correcte else "Exercice de rÃ©vision"
                },
                "encouragement": "Excellent! PrÃªt pour plus difficile?" if est_correcte else "Continue Ã  t'entraÃ®ner, tu vas y arriver!"
            })

        # ============ CHAT GÃ‰NÃ‰RIQUE CONTEXTUEL ==============
        
        # ðŸ”¥ NOUVEAU : RÃ©pondre diffÃ©remment selon le contexte rÃ©el de conversation
        if contexte_conversation.get("question_precedente"):
            prev_question = contexte_conversation["question_precedente"]
            
            # Si l'utilisateur dit "oui" aprÃ¨s une question
            if last_msg in ["oui", "ouais", "yep", "yes", "d'accord", "ok", "ok!"]:
                # RÃ©pondre avec l'action proposÃ©e
                if "exercice" in prev_question.lower():
                    n = 2
                    exercices = self._generer_exercices_contextuels(n, niveau, matiere)
                    return json.dumps({
                        "exercices": exercices,
                        "reponse": f"Super! Voici {n} exercices adaptÃ©s Ã  ton niveau pour pratiquer:",
                        "type": "exercice",
                        "niveau_adapte": True,
                        "confiance": 0.95
                    })
                elif "expliquer" in prev_question.lower() or "comprendre" in prev_question.lower():
                    return json.dumps({
                        "reponse": "Parfait! Dis-moi quel concept tu aimerais que j'explique plus en dÃ©tail.",
                        "type": "explication",
                        "niveau_adapte": True,
                        "confiance": 0.9
                    })
                else:
                    # RÃ©ponse gÃ©nÃ©rique mais contextuelle
                    return json.dumps({
                        "reponse": "Excellent! Je suis ravi. Qu'est-ce que tu aimerais faire maintenant? Tu peux me demander une explication, un exercice, ou une aide spÃ©cifique.",
                        "type": "explication",
                        "niveau_adapte": True,
                        "confiance": 0.85
                    })
            
            # Si l'utilisateur dit "non"
            elif last_msg in ["non", "non pas", "nope", "non merci", "non!"]:
                return json.dumps({
                    "reponse": "D'accord! Qu'est-ce que je peux faire pour t'aider alors? Je peux expliciter un concept, tester ta comprÃ©hension, ou te proposer une autre activitÃ©.",
                    "type": "explication",
                    "niveau_adapte": True,
                    "confiance": 0.85
                })

        # RÃ©ponse adaptÃ©e au niveau et contexte
        reponses_reflexes = {
            "salut": "Salut! Que puis-je t'expliquer en {matiere} pour le niveau {niveau}?",
            "merci": "De rien! N'hÃ©site pas Ã  demander si tu as d'autres questions!",
            "comment": "C'est une excellente question! Dans {matiere}, voici comment on pense Ã  Ã§a...",
            "pourquoi": "TrÃ¨s bonne question! La raison c'est que...",
            "difficile": "C'est normal que ce soit difficile! Tous les Ã©lÃ¨ves trouvent Ã§a complexe au dÃ©but. DÃ©composons-le Ã©tape par Ã©tape.",
            "facile": "Bravo! Tu progresses bien. Veux-tu passer Ã  quelque chose de plus difficile?",
            "comprends pas": "Ne t'inquiÃ¨te pas, je vais te l'expliquer diffÃ©remment. Dis-moi ce que tu as compris..."
        }

        reponse_base = f"C'est une bonne question! En {matiere.capitalize()} au niveau {niveau}, voici comment on peut l'aborder:"
        
        for trigger, response_template in reponses_reflexes.items():
            if trigger in prompt_lower:
                reponse_base = response_template
                break

        reponse = reponse_base.format(matiere=matiere.capitalize(), niveau=niveau)

        return json.dumps({
            "reponse": reponse + "\n\nVeux-tu un exercice pour pratiquer, on tu as d'autres questions?",
            "type": "explication",
            "niveau_adapte": True,
            "confiance": 0.88
        })

    def _generer_exercices_contextuels(self, n: int, niveau: str, matiere: str) -> List[Dict]:
        """GÃ©nÃ©rer des exercices vraiment contextuels"""
        
        # Banque d'exercices par niveau et matiÃ¨re
        banques = {
            "CM1-CM2": {
                "mathÃ©matiques": [
                    {"type": "num", "question": "Quel est le double de 25?", "options": ["50", "45", "52", "55"], "reponse": "50", "explication": "25 Ã— 2 = 50"},
                    {"type": "num", "question": "Divise 48 par 6", "options": ["6", "8", "7", "9"], "reponse": "8", "explication": "48 Ã· 6 = 8"},
                    {"type": "choix", "question": "Quelle est la fraction for 0.5?", "options": ["1/2", "1/3", "2/3", "1/4"], "reponse": "1/2", "explication": "0.5 = 50/100 = 1/2"},
                    {"type": "calcul", "question": "Calcule: 234 + 567", "options": ["801", "800", "802", "799"], "reponse": "801", "explication": "234 + 567 = 801"},
                    {"type": "prob", "question": "Quel est le pÃ©rimÃ¨tre d'un carrÃ© de 5cm?", "options": ["20cm", "15cm", "25cm", "10cm"], "reponse": "20cm", "explication": "PÃ©rimÃ¨tre = 4 Ã— cÃ´tÃ© = 4 Ã— 5 = 20cm"}
                ],
                "franÃ§ais": [
                    {"type": "gram", "question": "Quel est le verbe dans: 'Je suis heureux'?", "options": ["Je", "suis", "heureux", ""], "reponse": "suis", "explication": "'Suis' est le verbe (Ãªtre)"},
                    {"type": "gram", "question": "Accorde: 'Un chat (noir)'", "options": ["noir", "noirs", "noirÃ©", "noire"], "reponse": "noir", "explication": "Adjectif masculin singulier: noir"},
                    {"type": "ortho", "question": "ComplÃ¨te: 'C\\'est / C'est / Sait'", "options": ["C'est", "C\\'est", "Sait", "SÃ©"], "reponse": "C'est", "explication": "Contraction de 'ce est' = c'est"},
                    {"type": "conj", "question": "Conjugue 'avoir' au prÃ©sent (je)", "options": ["ai", "ais", "Ã¨", "h"], "reponsa": "ai", "explication": "Je ai = j'ai"}
                ]
            },
            "6Ã¨me-3Ã¨me": {
                "mathÃ©matiques": [
                    {"type": "algebra", "question": "RÃ©sous: 2x + 5 = 13", "options": ["4", "3", "5", "6"], "reponse": "4", "explication": "2x = 13 - 5 = 8, donc x = 4"},
                    {"type": "geo", "question": "Quel est le carrÃ© de 7?", "options": ["49", "48", "50", "64"], "reponsa": "49", "explication": "7Â² = 7 Ã— 7 = 49"},
                    {"type": "prob", "question": "Proba de tirer un as dans un jeu de 52 cartes?", "options": ["4/52", "1/13", "1/4", "4/48"], "reponse": "4/52", "explication": "Il y a 4 as sur 52 cartes"}
                ],
                "franÃ§ais": [
                    {"type": "gram", "question": "Type: 'Le chat que J'ai vu'", "options": ["principale", "subordonnÃ©e", "simple", "composÃ©e"], "reponse": "subordonnÃ©e", "explication": "'que j'ai vu' dÃ©pend de 'le chat'"},
                    {"type": "lit", "question": "Victor Hugo a Ã©crit?", "options": ["PhÃ¨dre", "Les MisÃ©rables", "Candide", "Dom Juan"], "reponse": "Les MisÃ©rables", "explication": "Victor Hugo (1802-1885) a Ã©crit Les MisÃ©rables"}
                ]
            },
            "2nde-Tle": {
                "mathÃ©matiques": [
                    {"type": "calc", "question": "DÃ©rivÃ©e de xÂ² + 3x est?", "options": ["2x + 3", "2x + 1", "x + 3", "2x"], "reponse": "2x + 3", "explication": "d/dx(xÂ²) = 2x, d/dx(3x) = 3"},
                    {"type": "log", "question": "log(100) en base 10 est?", "options": ["2", "3", "1", "4"], "reponse": "2", "explication": "10Â² = 100"}
                ],
                "philosophie": [
                    {"type": "philo", "question": "Descartes dit: 'Je pense donc...'", "options": ["je doute", "je suis", "je crois", "je peux"], "reponse": "je suis", "explication": "'Cogito ergo sum' - cela prouve l'existence de la conscience"},
                    {"type": "ethique", "question": "Utilitarisme = maximiser le?", "options": ["bonheur", "profit", "ordre", "pouvoir"], "reponsa": "bonheur", "explication": "L'utilitarisme cherche le plus grand bien pour le plus grand nombre"}
                ]
            }
        }

        # RÃ©cupÃ©rer la banque appropriÃ©e
        exercises = banques.get(niveau, {}).get(matiere, [])
        
        if not exercises:
            # Fallback gÃ©nÃ©rique mais rÃ©aliste
            exercises = [
                {
                    "type": "general",
                    "question": f"Question de rÃ©vision #{i} en {matiere}",
                    "options": ["Option A", "Option B", "Option C", "Option D"],
                    "reponse": "Option A",
                    "explication": "Explication pÃ©dagogique"
                }
                for i in range(1, 4)
            ]

        # SÃ©lectionner et formater
        result = []
        for i, ex in enumerate(exercises[:n], 1):
            result.append({
                "id": i,
                "type": ex.get("type", "choix_multiple"),
                "question": ex["question"],
                "options": ex.get("options", ["A", "B", "C", "D"]),
                "reponse": ex.get("reponse", ex.get("reponsa", "A")),
                "explication": ex.get("explication", "Voir le cours"),
                "erreurs_courantes": ["Confusion conceptuelle", "Erreur de calcul"],
                "difficulte": {"CM1-CM2": 3, "6Ã¨me-3Ã¨me": 5, "2nde-Tle": 8}.get(niveau, 5),
                "temps_estime": 120,
                "points": 10
            })
        
        return result

    def _evaluer_reponse_basique(self, reponse: str, prompt: str) -> bool:
        """Ã‰val basique mais intelligente"""
        # Mots-clÃ©s positifs
        positifs = ["correct", "juste", "bon", "exact", "vrai", "bien", "parfait"]
        negatifs = ["faux", "incorrec", "mauvais", "non", "erreur", "faux"]
        
        reponse_lower = reponse.lower()
        
        score_positif = sum(1 for p in positifs if p in reponse_lower)
        score_negatif = sum(1 for n in negatifs if n in reponse_lower)
        
        # Si c'est une rÃ©ponse chiffrÃ©e, tester cohÃ©rence basique
        if any(char.isdigit() for char in reponse):
            return True  # optimiste pour dÃ©mo
        
        return score_positif > score_negatif

    def generate_text(self, prompt: str) -> str:
        """GÃ©nÃ©rer du texte contextualisÃ©"""
        # Pour un vrai prompt, extraire des concepts et donner explica
        if "explication" in prompt.lower() or "explique" in prompt.lower():
            return "Bien sÃ»r! Voici une explication dÃ©taillÃ©e et adaptÃ©e Ã  ton niveau..."
        elif "recommanda" in prompt.lower():
            return json.dumps({
                "recommandations": [
                    "Exercice 1: Pratiquer les concepts clÃ©s",
                    "Exercice 2: Appliquer Ã  des cas rÃ©els",
                    "Projet: IntÃ©grer plusieurs concepts"
                ],
                "note": "BasÃ© sur ta progression actuelle"
            })
        
        return "Texte gÃ©nÃ©rÃ© intelligemment par ton tuteur."


# ========================
# MAIN LLM SERVICE
# ========================

class LLMService:
    """Service principal d'intÃ©gration LLM"""

    def __init__(self, provider: str = None):
        """
        Initialiser le service LLM

        Args:
            provider: "openai", "gemini", ou "mock"
        """
        # DÃ©terminer le provider Ã  partir de la variable d'environnement ou du paramÃ¨tre
        self.provider_name = provider or os.getenv("IA_PROVIDER", "openai")

        if self.provider_name == "openai":
            # OpenAIService lÃ¨ve une exception si la clÃ© n'est pas configurÃ©e
            self.service = OpenAIService()
        elif self.provider_name == "gemini":
            self.service = GeminiService()
        else:
            # Pour sÃ©curitÃ©, permettre explicitement mock mais ne pas l'utiliser par dÃ©faut
            self.service = MockLLMService()

    # ========================
    # TUTORING
    # ========================

    def chat_tuteur(
        self,
        message: str,
        niveau: str,
        matiere: str,
        age: int = 10,
        strengths: str = "aucune identifiÃ©e",
        weak_areas: str = "aucune identifiÃ©e",
        lecon_titre: str = "",
        lecon_contenu: str = "",
        conversation_history: List[Dict] = None,  # ðŸ”¥ NOUVEAU : Historique de conversation
    ) -> Dict[str, Any]:
        """
        Chat avec le tuteur IA. Utilise le niveau, la matiÃ¨re et optionnellement la leÃ§on en cours.
        
        Args:
            conversation_history: Liste complÃ¨te des messages antÃ©rieurs pour contexte
        """
        system = SYSTEM_PROMPT_TUTEUR.format(
            niveau=niveau or "non prÃ©cisÃ©",
            matiere=matiere or "gÃ©nÃ©ral",
            age=age,
            strengths=strengths,
            weak_areas=weak_areas,
            lecon_titre=lecon_titre or "aucune (conversation gÃ©nÃ©rale)",
            lecon_contenu=(lecon_contenu or "â€”")[:500],
        )

        # ðŸ”¥ Utiliser l'historique complet si disponible
        if conversation_history:
            messages = conversation_history
        else:
            messages = [{"role": "user", "content": message}]
            
        response = self.service.chat(messages, system)

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # DÃ©fensif: si le service renvoie du texte brut, Ã©viter d'echoer le message utilisateur
            raw = (response or "").strip()
            last_msg = (messages[-1].get('content') if messages else message) or message
            last_lower = (last_msg or "").lower()

            # Intent minimal: formule / volume
            if "volume" in last_lower or ("formule" in last_lower and "volume" in last_lower):
                reponse = (
                    "La formule du volume dÃ©pend de la forme. Par exemple:\n"
                    "- SphÃ¨re: V = 4/3 Ã— Ï€ Ã— rÂ³\n"
                    "- Cylindre: V = Ï€ Ã— rÂ² Ã— h\n"
                    "Souhaites-tu un exercice pour t'entraÃ®ner ?"
                )
                return {
                    "reponse": reponse,
                    "type": "explication",
                    "niveau_adapte": True,
                    "confiance": 0.95
                }

            # Salutation
            if any(w in last_lower for w in ["bonjour", "salut", "hello", "coucou"]):
                return {
                    "reponse": "Salut! Je suis ton tuteur. Dis-moi sur quel sujet tu veux travailler.",
                    "type": "salutation",
                    "niveau_adapte": True,
                    "confiance": 0.95
                }

            # Demande d'exercice -> tenter de gÃ©nÃ©rer au moins 1 exercice
            if any(w in last_lower for w in ["exercice", "exercices", "entraÃ®ne", "entrainement", "pratique", "gÃ©nÃ¨r", "genere"]):
                # Essayer d'utiliser le service interne si disponible
                try:
                    if hasattr(self.service, '_generer_exercices_contextuels'):
                        exercices = self.service._generer_exercices_contextuels(1, niveau or 'CM1-CM2', matiere or 'gÃ©nÃ©ral')
                        return {
                            "exercices": exercices,
                            "reponse": f"Voici {len(exercices)} exercice(s) pour pratiquer.",
                            "type": "exercice",
                            "niveau_adapte": True,
                            "confiance": 0.95
                        }
                except Exception:
                    pass

            # Fallback: ne jamais renvoyer l'input brut. Fournir une rÃ©ponse gÃ©nÃ©rique utile.
            fallback = (
                "Je n'ai pas compris complÃ¨tement ta demande. Peux-tu prÃ©ciser ? Par exemple: 'explique les fractions', 'gÃ©nÃ¨re des exercices', ou 'donne la formule du volume'."
            )
            return {
                "reponse": fallback,
                "type": "explication",
                "niveau_adapte": True,
                "confiance": 0.6
            }

    # ========================
    # EXERCICES
    # ========================

    def generer_exercices(
        self,
        nombre: int,
        niveau: str,
        matiere: str,
        topics: List[str],
        difficulty_history: str = "dÃ©butant"
    ) -> List[Dict[str, Any]]:
        """
        GÃ©nÃ©rer des exercices

        Args:
            nombre: Nombre d'exercices Ã  gÃ©nÃ©rer
            niveau: Niveau scolaire
            matiere: MatiÃ¨re
            topics: Topics Ã  couvrir
            difficulty_history: Historique de difficultÃ©

        Returns:
            Liste d'exercices gÃ©nÃ©rÃ©s
        """
        system = SYSTEM_PROMPT_EXERCICE_GENERATOR.format(
            niveau=niveau,
            matiere=matiere,
            nombre=nombre,
            topics=", ".join(topics),
            difficulty_history=difficulty_history
        )

        prompt = f"GÃ©nÃ¨re {nombre} exercices pour {matiere} au niveau {niveau} sur {', '.join(topics)}"
        messages = [{"role": "user", "content": prompt}]

        response = self.service.chat(messages, system)
        try:
            data = json.loads(response)
            exercices = data.get("exercices", [])
            # Garantir au moins 1 exercice
            if not exercices:
                raise ValueError("Aucun exercice retournÃ© par le modÃ¨le")
            return exercices
        except (json.JSONDecodeError, ValueError):
            # Tentative de secours: utiliser le gÃ©nÃ©rateur interne si disponible
            try:
                if hasattr(self.service, '_generer_exercices_contextuels'):
                    return self.service._generer_exercices_contextuels(max(1, nombre), niveau or 'CM1-CM2', matiere or 'gÃ©nÃ©ral')
            except Exception:
                pass
            # Fallback final: gÃ©nÃ©rer un exercice gÃ©nÃ©rique simple
            return [
                {
                    "id": 1,
                    "type": "general",
                    "question": f"Exercice de rÃ©vision en {matiere} (niveau {niveau}) : Donne un exemple.",
                    "options": [],
                    "reponse": "Exemple attendu",
                    "explication": "Ceci est un exercice gÃ©nÃ©rÃ© par le systÃ¨me car le service LLM n'a pas renvoyÃ© d'exercices."
                }
            ]

    # ========================
    # ANALYSE
    # ========================

    def analyser_reponse(
        self,
        question: str,
        reponse_donnee: str,
        reponse_correcte: str,
        concept: str,
        niveau: str
    ) -> Dict[str, Any]:
        """
        Analyser une rÃ©ponse intelligemment

        Args:
            question: Question posÃ©e
            reponse_donnee: RÃ©ponse donnÃ©e par l'Ã©lÃ¨ve
            reponse_correcte: RÃ©ponse correcte
            concept: Concept enseignÃ©
            niveau: Niveau de l'Ã©lÃ¨ve

        Returns:
            Analyse dÃ©taillÃ©e
        """
        system = SYSTEM_PROMPT_ANALYSIS

        prompt = f"""
        Question: {question}
        Concept: {concept}
        
        RÃ©ponse de l'Ã©lÃ¨ve: {reponse_donnee}
        RÃ©ponse correcte: {reponse_correcte}
        
        Niveau de l'Ã©lÃ¨ve: {niveau}
        
        Analyse cette rÃ©ponse et fournis un feedback dÃ©taillÃ© et encourageant.
        """

        messages = [{"role": "user", "content": prompt}]
        response = self.service.chat(messages, system)

        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {
                "correct": reponse_donnee.strip().lower() == reponse_correcte.strip().lower(),
                "score": 100 if reponse_donnee.strip().lower() == reponse_correcte.strip().lower() else 0,
                "feedback_positif": "Bonne tentative!",
                "explication": f"La rÃ©ponse correcte Ã©tait: {reponse_correcte}",
                "encouragement": "Continue Ã  progresser!"
            }

    # ========================
    # EXPLICATIONS
    # ========================

    def expliquer_concept(
        self,
        concept: str,
        niveau: str,
        matiere: str,
        style: str = "analogie"  # analogie, exemple, technique
    ) -> str:
        """
        Expliquer un concept

        Args:
            concept: Concept Ã  expliquer
            niveau: Niveau de l'Ã©lÃ¨ve
            matiere: MatiÃ¨re
            style: Style d'explication

        Returns:
            Explication adaptÃ©e
        """
        prompt = f"""
        Explique le concept '{concept}' en {matiere} pour un Ã©lÃ¨ve de niveau {niveau}.
        Style d'explication: {style} (utilise des analogies, des exemples concrets, ou des dÃ©tails techniques).
        Sois clair et adaptÃ© au niveau de l'Ã©lÃ¨ve.
        """

        return self.service.generate_text(prompt)

    # ========================
    # RECOMMANDATIONS
    # ========================

    def recommander_contenu(
        self,
        niveau: str,
        matiere: str,
        weak_areas: List[str],
        strengths: List[str]
    ) -> Dict[str, Any]:
        """
        Recommander du contenu personnalisÃ©

        Args:
            niveau: Niveau de l'Ã©lÃ¨ve
            matiere: MatiÃ¨re
            weak_areas: Domaines faibles
            strengths: Points forts

        Returns:
            Recommandations
        """
        prompt = f"""
        Pour un Ã©lÃ¨ve de niveau {niveau} en {matiere}:
        - Points forts: {', '.join(strengths)}
        - A amÃ©liorer: {', '.join(weak_areas)}
        
        Recommande:
        1. Les 3 prochaines leÃ§ons Ã  Ã©tudier
        2. 2 exercices complÃ©mentaires
        3. 1 projet d'approfondissement
        """

        response = self.service.generate_text(prompt)

        return {
            "recommandations": response,
            "generated_at": None
        }


# ========================
# SINGLETON GLOBAL
# ========================

_llm_service_cache = {}


def get_llm_service(provider: str = None) -> LLMService:
    """Obtenir le service LLM correspondant au provider (relire IA_PROVIDER Ã  chaque appel)"""
    provider = provider or os.getenv("IA_PROVIDER", "openai")
    
    # Cacher une instance par provider, mais relire IA_PROVIDER chaque fois
    if provider not in _llm_service_cache:
        _llm_service_cache[provider] = LLMService(provider)
    return _llm_service_cache[provider]
